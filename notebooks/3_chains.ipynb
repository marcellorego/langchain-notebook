{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1_chains_basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q python-dotenv langchain langchain-community langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from .env\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOllama(\n",
    "        base_url=\"http://localhost:11434\",\n",
    "        model=\"llama3.2\",\n",
    "        temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt templates (no need for separate Runnable chains)\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
    "        (\"human\", \"Tell me {joke_count} jokes.\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the combined chain using LangChain Expression Language (LCEL)\n",
    "chain = prompt_template | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the combined chain using .pipe() method\n",
    "chain = prompt_template.pipe(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the chain\n",
    "result = chain.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Here are three lawyer-themed jokes for you:\\n\\n1. Why did the lawyer\\'s client bring a ladder to the courtroom?\\n\\nBecause he wanted to take his case to a higher court! (get it?)\\n\\n2. Why did the lawyer\\'s wife leave him?\\n\\nBecause she couldn\\'t take his constant \"objection\" to their relationship!\\n\\n3. What did the lawyer say to the coffee machine in the office break room?\\n\\n\"You\\'re always brewing up trouble, but I\\'m going to have to sue you for emotional distress!\"' additional_kwargs={} response_metadata={'model': 'llama3.2', 'created_at': '2024-11-10T21:31:24.060522506Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 10342761537, 'load_duration': 27722983, 'prompt_eval_count': 41, 'prompt_eval_duration': 97000000, 'eval_count': 104, 'eval_duration': 10216000000} id='run-26e78236-fc05-4350-97e4-1cbf71cce7c8-0' usage_metadata={'input_tokens': 41, 'output_tokens': 104, 'total_tokens': 145}\n"
     ]
    }
   ],
   "source": [
    "# Output\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the combined chain using LangChain Expression Language (LCEL)\n",
    "chain = prompt_template | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Here are three lawyer-themed jokes for you:\\n\\n1. Why did the lawyer\\'s client bring a ladder to the courtroom?\\n\\nBecause he wanted to take his case to a higher court! (get it?)\\n\\n2. Why did the lawyer\\'s wife leave him?\\n\\nBecause she couldn\\'t take his constant \"objection\" to their relationship!\\n\\n3. What did the lawyer say when his client asked him how much it would cost to sue someone?\\n\\n\"Don\\'t worry, we\\'ll bill you by the hour... and then some!\"' additional_kwargs={} response_metadata={'model': 'llama3.2', 'created_at': '2024-11-10T21:29:53.986792135Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 11411358597, 'load_duration': 20991038, 'prompt_eval_count': 41, 'prompt_eval_duration': 997000000, 'eval_count': 104, 'eval_duration': 10391000000} id='run-60ea568d-e7f3-4f21-b59f-fa0fe4ccc684-0' usage_metadata={'input_tokens': 41, 'output_tokens': 104, 'total_tokens': 145}\n"
     ]
    }
   ],
   "source": [
    "# Output\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2_chains_under_the_hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda, RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create individual runnables (steps in the chain)\n",
    "format_prompt = RunnableLambda(lambda x: prompt_template.format_prompt(**x))\n",
    "invoke_model = RunnableLambda(lambda x: model.invoke(x.to_messages()))\n",
    "parse_output = RunnableLambda(lambda x: x.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RunnableSequence (equivalent to the LCEL chain)\n",
    "chain = RunnableSequence(first=format_prompt, middle=[invoke_model], last=parse_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the chain\n",
    "response = chain.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are three lawyer-themed jokes for you:\n",
      "\n",
      "1. Why did the lawyer's client bring a ladder to the courtroom?\n",
      "\n",
      "Because he wanted to take his case to a higher court! (get it?)\n",
      "\n",
      "2. Why did the lawyer's wife leave him?\n",
      "\n",
      "Because she couldn't take his constant \"objection\" to their relationship!\n",
      "\n",
      "3. What did the lawyer say to the coffee machine in the office break room?\n",
      "\n",
      "\"You're always brewing up trouble, but I'm going to have to sue you for emotional distress!\"\n"
     ]
    }
   ],
   "source": [
    "# Output\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3_chains_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define additional processing steps using RunnableLambda\n",
    "uppercase_output = RunnableLambda(lambda x: x.upper())\n",
    "count_words = RunnableLambda(lambda x: f\"Word count: {len(x.split())}\\n{x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the combined chain using LangChain Expression Language (LCEL)\n",
    "chain = prompt_template | model | StrOutputParser() | uppercase_output | count_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the chain\n",
    "result = chain.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count: 81\n",
      "HERE ARE THREE LAWYER-THEMED JOKES FOR YOU:\n",
      "\n",
      "1. WHY DID THE LAWYER'S CLIENT BRING A LADDER TO THE COURTROOM?\n",
      "\n",
      "BECAUSE HE WANTED TO TAKE HIS CASE TO A HIGHER COURT! (GET IT?)\n",
      "\n",
      "2. WHY DID THE LAWYER'S WIFE LEAVE HIM?\n",
      "\n",
      "BECAUSE SHE COULDN'T TAKE HIS CONSTANT \"OBJECTION\" TO THEIR RELATIONSHIP!\n",
      "\n",
      "3. WHAT DID THE LAWYER SAY TO THE COFFEE MACHINE IN THE OFFICE BREAK ROOM?\n",
      "\n",
      "\"YOU'RE ALWAYS BREWING UP TROUBLE, BUT I'M GOING TO HAVE TO SUE YOU FOR EMOTIONAL DISTRESS!\"\n"
     ]
    }
   ],
   "source": [
    "# Output\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4_chains_parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnableParallel, RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an expert product reviewer.\"),\n",
    "        (\"human\", \"List the main features of the product {product_name}.\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pros analysis step\n",
    "def analyze_pros(features):\n",
    "    pros_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are an expert product reviewer.\"),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Given these features: {features}, list the pros of these features.\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return pros_template.format_prompt(features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cons analysis step\n",
    "def analyze_cons(features):\n",
    "    cons_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are an expert product reviewer.\"),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Given these features: {features}, list the cons of these features.\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return cons_template.format_prompt(features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine pros and cons into a final review\n",
    "def combine_pros_cons(pros, cons):\n",
    "    return f\"Pros:\\n{pros}\\n\\nCons:\\n{cons}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify branches with LCEL\n",
    "pros_branch_chain = (\n",
    "    RunnableLambda(lambda x: analyze_pros(x)) | model | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons_branch_chain = (\n",
    "    RunnableLambda(lambda x: analyze_cons(x)) | model | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the combined chain using LangChain Expression Language (LCEL)\n",
    "chain = (\n",
    "    prompt_template\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    "    | RunnableParallel(branches={\"pros\": pros_branch_chain, \"cons\": cons_branch_chain})\n",
    "    | RunnableLambda(lambda x: combine_pros_cons(x[\"branches\"][\"pros\"], x[\"branches\"][\"cons\"]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the chain\n",
    "result = chain.invoke({\"product_name\": \"MacBook Pro\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pros:\n",
      "Based on the provided features, here are some pros of the MacBook Pro:\n",
      "\n",
      "1. **High-Resolution Display**: The Retina display with high-resolution (up to 5K) and vibrant colors provides an excellent visual experience for watching videos, browsing the web, and working on creative projects.\n",
      "\n",
      "2. **Durable Aluminum Unibody Construction**: The aluminum unibody construction is both durable and lightweight, making it easy to carry around without sacrificing performance or protection.\n",
      "\n",
      "3. **Long Battery Life**: With up to 20 hours of web browsing on a single charge, the MacBook Pro offers an impressive battery life that can last throughout a full day, even with heavy usage.\n",
      "\n",
      "4. **Fast Charging Capabilities**: The fast charging capabilities allow for quick top-ups, making it convenient to keep your laptop charged and ready to go whenever you need it.\n",
      "\n",
      "5. **High-Performance Computing**: With up to 10th or 11th Gen Intel Core i9 processor, the MacBook Pro offers high-performance computing capabilities that can handle demanding tasks like video editing, 3D modeling, and software development.\n",
      "\n",
      "6. **Ample Storage Options**: The availability of up to 64GB of RAM and up to 4TB of SSD storage provides ample storage options for users who need to store large files, projects, or applications.\n",
      "\n",
      "7. **Improved Audio and Camera Quality**: The stereo speakers with improved sound quality and the high-resolution FaceTime HD camera with advanced features like TrueDepth technology provide an excellent audio and video experience.\n",
      "\n",
      "8. **Stable and Secure Operating System**: The macOS operating system is known for its stability, security, and user-friendly interface, making it a great choice for users who value reliability and ease of use.\n",
      "\n",
      "9. **Support for External Displays**: The support for external displays up to 6K resolution allows users to connect multiple monitors or projectors, making it ideal for professionals who need to work with multiple screens.\n",
      "\n",
      "10. **Convenient Touch Bar and Touch ID (on some models)**: The touch bar and touch ID feature provide a convenient way to access frequently used functions and authenticate devices, adding an extra layer of convenience and security to the laptop experience.\n",
      "\n",
      "Cons:\n",
      "While the MacBook Pro has many impressive features, here are some potential downsides to consider:\n",
      "\n",
      "**Design and Display**\n",
      "\n",
      "* The aluminum unibody construction can be heavy and hot to the touch\n",
      "* Some users may find the weight (3-4 pounds) too much for their liking\n",
      "* The dimensions (12.3 x 8.6 x 0.61 inches) may not fit comfortably in some bags or sleeves\n",
      "\n",
      "**Performance**\n",
      "\n",
      "* High-performance computing comes at a cost, and the MacBook Pro can be quite expensive\n",
      "* Up to 64GB of RAM is overkill for most users, making it a costly upgrade\n",
      "* The integrated graphics (Intel Iris Xe Graphics or AMD Radeon Pro 560X) may not be sufficient for demanding tasks like gaming\n",
      "\n",
      "**Battery Life**\n",
      "\n",
      "* While up to 20 hours of web browsing is impressive, actual battery life may vary depending on usage patterns\n",
      "* Fast charging capabilities can be convenient, but they also come with a higher price tag\n",
      "* Some users may find the battery life too short for their needs, especially if they use their laptop extensively\n",
      "\n",
      "**Ports and Connectivity**\n",
      "\n",
      "* The lack of USB-A ports can be frustrating for users who need to connect older devices\n",
      "* While Thunderbolt 3 is fast, it's not as widely supported as other ports like USB-C or HDMI\n",
      "* Wi-Fi 6 (802.11ax) connectivity may not offer significant performance improvements over Wi-Fi 5 (802.11ac)\n",
      "\n",
      "**Camera and Audio**\n",
      "\n",
      "* The high-resolution FaceTime HD camera can be prone to glare and reflections\n",
      "* Stereo speakers, while improved, may still lack the depth and clarity of dedicated audio systems\n",
      "\n",
      "**Operating System**\n",
      "\n",
      "* macOS is known for its stability and security, but it can also be restrictive and less customizable than other operating systems\n",
      "* Some users may find the learning curve steeper due to Apple's proprietary software and hardware integration\n",
      "\n",
      "**Additional Features**\n",
      "\n",
      "* The Touch Bar and Touch ID (on some models) can be gimmicky or unnecessary for some users\n",
      "* Backlit keyboards, while convenient, can also be distracting in low-light environments\n",
      "* Support for external displays up to 6K resolution may not be necessary for most users, adding to the overall cost\n",
      "\n",
      "**Other Considerations**\n",
      "\n",
      "* The MacBook Pro's lack of a headphone jack and limited expansion options (e.g., no SD card slot) may limit its versatility\n",
      "* Some users may find the keyboard and trackpad layout too cramped or uncomfortable for extended use.\n",
      "\n",
      "Keep in mind that these cons are subjective and may vary depending on individual needs and preferences.\n"
     ]
    }
   ],
   "source": [
    "# Output\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5_chains_branching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableBranch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt templates for different feedback types\n",
    "positive_feedback_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\",\n",
    "         \"Generate a thank you note for this positive feedback: {feedback}.\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_feedback_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\",\n",
    "         \"Generate a response addressing this negative feedback: {feedback}.\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_feedback_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Generate a request for more details for this neutral feedback: {feedback}.\",\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "escalate_feedback_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Generate a message to escalate this feedback to a human agent: {feedback}.\",\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feedback classification template\n",
    "classification_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\",\n",
    "         \"Classify the sentiment of this feedback as positive, negative, neutral, or escalate: {feedback}.\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the runnable branches for handling feedback\n",
    "branches = RunnableBranch(\n",
    "    (\n",
    "        lambda x: \"positive\" in x,\n",
    "        positive_feedback_template | model | StrOutputParser()  # Positive feedback chain\n",
    "    ),\n",
    "    (\n",
    "        lambda x: \"negative\" in x,\n",
    "        negative_feedback_template | model | StrOutputParser()  # Negative feedback chain\n",
    "    ),\n",
    "    (\n",
    "        lambda x: \"neutral\" in x,\n",
    "        neutral_feedback_template | model | StrOutputParser()  # Neutral feedback chain\n",
    "    ),\n",
    "    escalate_feedback_template | model | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the classification chain\n",
    "classification_chain = classification_template | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine classification and response generation into one chain\n",
    "chain = classification_chain | branches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the chain with an example review\n",
    "# Good review - \"The product is excellent. I really enjoyed using it and found it very helpful.\"\n",
    "# Bad review - \"The product is terrible. It broke after just one use and the quality is very poor.\"\n",
    "# Neutral review - \"The product is okay. It works as expected but nothing exceptional.\"\n",
    "# Default - \"I'm not sure about the product yet. Can you tell me more about its features and benefits?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"The product is terrible. It broke after just one use and the quality is very poor.\"\n",
    "result = chain.invoke({\"feedback\": review})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a sample message you can use to escalate the feedback to a human agent:\n",
      "\n",
      "\"Hi [Agent's Name], \n",
      "\n",
      "I'm reaching out regarding a recent interaction with your system. The user provided feedback that I've classified as 'Negative'. I'd like to discuss this further and explore ways to improve our response to similar situations.\n",
      "\n",
      "Could we schedule a call to review the feedback and discuss potential next steps? I'd appreciate the opportunity to provide more context and get your input on how to address this issue.\n",
      "\n",
      "Please let me know a convenient time for you, or if there's any additional information you need from me beforehand.\n",
      "\n",
      "Thank you,\n",
      "[Your Name]\"\n",
      "\n",
      "This message provides context, explains the reason for escalating the feedback, and requests a call with the agent to discuss further.\n"
     ]
    }
   ],
   "source": [
    "# Output the result\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
