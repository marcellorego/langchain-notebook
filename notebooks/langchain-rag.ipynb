{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a80cb37-da18-41c7-8c42-1022abb66727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet --upgrade langchain langchain-community langchain-chroma langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96109e8f-d6e2-4396-b57d-ecb8a483ea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "model = OllamaLLM(base_url=\"http://localhost:11434\", model=\"llama3.2\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec8a0a87-5d6a-48f7-aa81-d3c8cccb2f05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I\\'m an AI, and I don\\'t have a specific \"version\" of llama in the classical sense. However, I was trained on a large dataset of text that includes information about llamas.\\n\\nMy training data is based on a snapshot of the internet from 2021, and it\\'s constantly updated to keep my knowledge up-to-date. This means that my responses may not reflect the very latest developments or research in the field of llama biology or behavior.\\n\\nThat being said, I can provide you with information about llamas, including their history, behavior, diet, and more. If you have a specific question about llamas, feel free to ask!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Who are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b2f7c72-244d-4e01-b670-f1cb4e1d487e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://python.langchain.com/docs/introduction/\")\n",
    "loader.requests_kwargs = {'verify':False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36c51cdf-8d90-40bc-868d-1653bdfa599e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrego/Projects/workspace/langchain-notebook/.venv/lib/python3.10/site-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'python.langchain.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "218b28a5-7e32-449c-9ab8-05a5463b939a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nIntroduction | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentIntegrationsAPI ReferenceMoreContributingPeopleLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a Simple LLM Application with LCELBuild a Query Analysis SystemBuild a ChatbotConversational RAGBuild an Extraction ChainBuild an AgentTaggingdata_generationBuild a Local RAG ApplicationBuild a PDF ingestion and Question/Answering systemBuild a Retrieval Augmented Generation (RAG) AppVector stores and retrieversBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to map values to a graph databaseHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to best prompt for Graph-RAGHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to split by HTML headerHow to split by HTML sectionsHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables as ToolsHow to create custom callback handlersHow to create a custom chat model classHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurityIntroductionOn this pageIntroduction\\nLangChain is a framework for developing applications powered by large language models (LLMs).\\nLangChain simplifies every stage of the LLM application lifecycle:\\n\\nDevelopment: Build your applications using LangChain\\'s open-source building blocks, components, and third-party integrations.\\nUse LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your chains, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Cloud.\\n\\n\\n\\nConcretely, the framework consists of the following open-source libraries:\\n\\nlangchain-core: Base abstractions and LangChain Expression Language.\\nlangchain-community: Third party integrations.\\n\\nPartner packages (e.g. langchain-openai, langchain-anthropic, etc.): Some integrations have been further split into their own lightweight packages that only depend on langchain-core.\\n\\n\\nlangchain: Chains, agents, and retrieval strategies that make up an application\\'s cognitive architecture.\\nLangGraph: Build robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph. Integrates smoothly with LangChain, but can be used without it.\\nLangServe: Deploy LangChain chains as REST APIs.\\nLangSmith: A developer platform that lets you debug, test, evaluate, and monitor LLM applications.\\n\\nnoteThese docs focus on the Python LangChain library. Head here for docs on the JavaScript LangChain library.\\nTutorials\\u200b\\nIf you\\'re looking to build something specific or are more of a hands-on learner, check out our tutorials section.\\nThis is the best place to get started.\\nThese are the best ones to get started with:\\n\\nBuild a Simple LLM Application\\nBuild a Chatbot\\nBuild an Agent\\nIntroduction to LangGraph\\n\\nExplore the full list of LangChain tutorials here, and check out other LangGraph tutorials here. To learn more about LangGraph, check out our first LangChain Academy course, Introduction to LangGraph, available here.\\nHow-to guides\\u200b\\nHere you‚Äôll find short answers to ‚ÄúHow do I‚Ä¶.?‚Äù types of questions.\\nThese how-to guides don‚Äôt cover topics in depth ‚Äì you‚Äôll find that material in the Tutorials and the API Reference.\\nHowever, these guides will help you quickly accomplish common tasks.\\nCheck out LangGraph-specific how-tos here.\\nConceptual guide\\u200b\\nIntroductions to all the key parts of LangChain you‚Äôll need to know! Here you\\'ll find high level explanations of all LangChain concepts.\\nFor a deeper dive into LangGraph concepts, check out this page.\\nAPI reference\\u200b\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\nEcosystem\\u200b\\nü¶úüõ†Ô∏è LangSmith\\u200b\\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\nü¶úüï∏Ô∏è LangGraph\\u200b\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it.\\nAdditional resources\\u200b\\nVersions\\u200b\\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\\nSecurity\\u200b\\nRead up on security best practices to make sure you\\'re developing safely with LangChain.\\nIntegrations\\u200b\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. Check out our growing list of integrations.\\nContributing\\u200b\\nCheck out the developer\\'s guide for guidelines on contributing and help getting your dev environment set up.Edit this pageWas this page helpful?NextTutorialsTutorialsHow-to guidesConceptual guideAPI referenceEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphAdditional resourcesVersionsSecurityIntegrationsContributingCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\\n\\n')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d5ba525-5ad8-4b7d-9212-20db86e93638",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\",)\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f5e348d-e08c-48f0-bc5b-1dbb310aa2ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e701d307-cdaa-4377-b60e-2449dfcabb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e', 'title': 'LLM RAG Paradigms: Naive RAG, Advanced RAG & Modular RAG | by Dr Julija | Medium', 'description': 'Here I describe my key learnings on how RAG systems evolved over the last few years. I share the differences between Naive RAG, Advanced RAG and Modular RAG frameworks. I summarize key insights from‚Ä¶', 'language': 'en'}, page_content='LLM RAG Paradigms: Naive RAG, Advanced RAG & Modular RAG | by Dr Julija | MediumOpen in appSign upSign inWriteSign upSign inLLM RAG Paradigms: Naive RAG, Advanced RAG & Modular RAGDr Julija¬∑Follow6 min read¬∑Mar 10, 2024--2ListenShareThree RAG Paradigms | üìî DrJulija‚Äôs Notebooküìù OverviewHere I describe my key learnings on how RAG systems evolved over the last few years. I share the differences between Naive RAG, Advanced RAG and Modular RAG frameworks. I summarize key insights from a great RAG technology survey paper Gao et al. 2024.üõ† What is a RAG Framework?Large Language Models (LLMs) such as the GPT series from OpenAI, LLama series by Meta, and Gemini by Google have achieved significant achievements in the generative AI field.But these models are non deterministic. Often, LLMs may produce content that is either inaccurate or irrelevant (known as hallucinations), rely on outdated information, and their decision-making processes are not transparent, leading to black-box reasoning.Retrieval-Augmented Generation (RAG) framework is designed to help mitigate these challenges. RAG enhances LLMs‚Äô knowledge base with additional, domain-specific data.For example, RAG-based systems are used in advanced question-answering (Q&A) applications ‚Äî chatbots. To create a chatbot that can understand and respond to queries about private or specific topics, it‚Äôs necessary to expand the knowledge of Large Language Models (LLMs) with the particular data needed. This is where the RAG can help.üîó Read about how I built a Naive RAG pipeline HERE.üë©üèª\\u200düíª Naive RAG, Advanced RAG & Modular RAGRAG framework addresses the following questions:‚ÄúWhat to retrieve‚Äù‚ÄúWhen to retrieve‚Äù‚ÄúHow to use the retrieved information‚ÄùOver the last few years there has been a lot of research and innovation in the RAG space. RAG systems can be split into 3 categories:Naive RAGAdvanced RAGModular RAGSee the comparison between all three paradigms of RAG ‚Äî Naive RAG, Advanced RAG and Modular RAG below.Comparison between the three paradigms of RAG (Gao et al. 2024)1Ô∏è‚É£ Naive RAGThe Naive RAG pipeline consists of the below key phases:Data IndexingData Loading: This involves importing all the documents or information to be utilized.Data Splitting: Large documents are divided into smaller pieces, for instance, sections of no more than 500 characters each.Data Embedding: The data is converted into vector form using an embedding model, making it understandable for computers.Data Storing: These vector embeddings are saved in a vector database, allowing them to be easily searched.RetrievalWhen a user asks a question:The user‚Äôs input is first transformed into a vector (query vector) using the same embedding model from the Data Indexing phase.This query vector is then matched against all vectors in the vector database to find the most similar ones (e.g., using the Euclidean distance metric) that might contain the answer to the user‚Äôs question. This step is about identifying relevant knowledge chunks.Augmentation & GenerationThe LLM model takes the user‚Äôs question and the relevant information retrieved from the vector database to create a response. This process combines the question with the identified data (augmentation) to generate an answer (generation).‚úã Problems with Naive RAGNaive RAG faces challenges across all phases:Retrieval ‚Äî failure to retrieve all relevant chunks or retrieving irrelevant chunks.Augmentation ‚Äî challenges with integrating the context from retrieved chunks that may be disjointed or contain repetitive information.Generation ‚Äî LLM may potentially generate answers that are not grounded in the provided context (retrieved chunks) or generate answers based on an irrelevant context that is retrieved.2Ô∏è‚É£ Advanced RAGAdvanced RAG strategies have been developed to address the challenges faced by Naive RAG. Below is an overview of key Advanced RAG techniques.RAG applications must efficiently retrieve relevant documents from the data source. But there are multiple challenges in each step.How can we achieve accurate semantic representations of documents and queries?What methods can align the semantic spaces of queries and documents (chunks)?How can the retriever‚Äôs output be aligned with the preferences of the LLM?Here I give an overview of pre-retrieval, retrieval and post-retrieval strategies:‚û°Ô∏è Pre-RetrievalHow to optimize the data indexing?Improve Data Quality ‚Äî remove irrelevant information, removing ambiguity in entities and terms, confirming factual accuracy, maintaining context, and updating outdated information.Optimize Index Structure ‚Äî optimize chunk sizes to capture relevant context or add information from graph structure to capture relationships between entities.Add Metadata ‚Äî add dates, chapters, subsections, purposes or any other relevant information into chunks as metadata to improve the data filteringChunk Optimization ‚Äî when using external data sources / documents to build RAG pipeline, the initial step is break them down into smaller chunks to extract fine- grained features. Chunks are then embedded to represent their semantics. But embedding too large or too small text chunks may lead to sub-optimal outcome therefore we need to optimize chunk size for the types of documents we use in the RAG pipeline.üìù Summary of Key Pre-Retrieval Techniques:Sliding Window ‚Äî chunking method that uses overlap between the chunks.Auto-Merging Retrieval ‚Äî utilizes small text blocks during the initial search phase and subsequently provides larger related text blocks to the language model for processing.Abstract Embedding ‚Äî prioritizes Top-K retrieval based on document abstracts (or summaries), offering a comprehensive understanding of the entire document context.Metadata Filtering ‚Äî leverages document metadata to enhance the filtering process.Graph Indexing ‚Äî transforms entities and relationships into nodes and connections, significantly improving relevance.‚û°Ô∏è RetrievalOnce the size of chunks is determined, the next step is to embed these chunks into the semantic space using an embedding model.During the retrieval stage, the goal is to identify the most relevant chunks to query. This is done by calculating the similarity between the query and chunks. Here, we can optimize embedding models that are used to embed both the query and chunks.Domain Knowledge Fine-Tuning ‚Äî to ensure that an embedding model accurately captures domain-specific information of the RAG system, it is important to use domain-specific datasets for fine-tuning. The dataset for embedding model fine-tuning should contain: queries, a corpus and relevant documents.Similarity Metrics ‚Äî there are a number of different metrics to measure similarity between the vectors. The choise of the similarity metric is also an optimization problem. Vector databases (ChromaDB, Pinecode, Weaviate‚Ä¶) support multiple similarity metrics. Here a few examples of different similarity metrics:Cosine SimilarityEuclidean Distance (L2)Dot ProductL2 Squared DistanceManhattan Distance‚û°Ô∏è Post-RetrievalAfter retrieving the context data (chunks) from a vector database, the next step is to merge the context with a query as an input into LLM. But some of the retrieved chunks may be repeated, noisy or contain irrelevant information. This may have an impact on how LLM processes the given context.Below I list a few strategies used to overcome these issues.Reranking ‚Äî rerank the retrieved information to prioritize the most relevant content first. LLMs often face performance declines when additional context is introduced, and reranking addresses this issue by reranking the retrieved chunks and identiying Top-K most relevant chunks that are then used as a context in LLM. Libraries such as LlamaIndex, Langchain, HayStack offer different rerankers.Prompt Compression ‚Äî retrieved information might be noisy, its important to compress irrelevant context and reduce context length before presenting to LLM. Use Small Language Models to calculate prompt mutual information or perplexity to estimate element importance. Use summarization techniques when the context is long.3Ô∏è‚É£ Modular RAGModular RAG integrates various modules and techniques from Adanced RAG to improve the overall RAG system. For example, incorporating a search module for similarity retrieval and applying a fine-tuning approach in the retriever. Modular RAG became a standard paradigm when building RAG applications. A few example of modules:Search Module ‚Äî in addition to retrieving context from vector database, search modules intergrates data from other sources such as search engines, tabular data, knowledge graphs etc.Memory Module ‚Äî adding memory component into RAG system where LLM can refer not only to the chunks retrieved from the vector database but also to the previous queries and answers that are stored in the systems memory.Fusion ‚Äî involves parallel vector searches of both original and expanded queries, intelligent reranking to optimize results, and pairing the best outcomes with new queries.Routing ‚Äî query routing decides the subsequent action to a user‚Äôs query for example summarization, searching specific databases, etc.üîó Read about how I built a Naive RAG pipeline HERE.RagArtificial IntelligenceLlmNLPRag Optimization----2FollowWritten by Dr Julija112 FollowersFounder of MiniMe ai [myminime.ai] and Networky [networky.co] | AI Engineer | Entrepreneur | PhD | Machine Learning | NLP | Artificial IntelligenceFollowHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fe2cce4-cf83-4621-9c8d-824612a0228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0580774d-8b8d-4100-aba5-60a6fb7ae175",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = embeddings.embed_documents([\"Sample test for embedding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6164766-28f8-496e-92c1-397b87d6faad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c38ea06e-b083-489b-b307-4e04d1766684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.010836153,\n",
       " 0.046975333,\n",
       " -0.16885464,\n",
       " -0.083269686,\n",
       " 0.046267815,\n",
       " -0.025845231,\n",
       " 0.048265185,\n",
       " 0.00015788786,\n",
       " -0.0029027986,\n",
       " -0.025514128,\n",
       " -0.039691143,\n",
       " 0.014862314,\n",
       " 0.04225586,\n",
       " 0.03155704,\n",
       " -0.08613685,\n",
       " 0.031448245,\n",
       " 0.055130716,\n",
       " -0.08424685,\n",
       " -0.0300519,\n",
       " 0.023034247,\n",
       " -0.02893675,\n",
       " -0.011188595,\n",
       " -0.04197221,\n",
       " 0.01657333,\n",
       " 0.08122299,\n",
       " -0.025474742,\n",
       " 0.020334475,\n",
       " 0.031586822,\n",
       " -0.060741425,\n",
       " -0.024510806,\n",
       " 0.025406715,\n",
       " -0.035421725,\n",
       " 0.026300289,\n",
       " -0.09746591,\n",
       " -0.017988276,\n",
       " -0.023607237,\n",
       " 0.013738853,\n",
       " 0.057773974,\n",
       " -0.012506569,\n",
       " 0.025471868,\n",
       " -0.0109925065,\n",
       " 0.021549238,\n",
       " -0.0264301,\n",
       " 0.0049314494,\n",
       " 0.012061574,\n",
       " 0.0024457255,\n",
       " 0.071359776,\n",
       " -0.004695334,\n",
       " 0.050568372,\n",
       " -0.064182475,\n",
       " -0.005945119,\n",
       " -0.0069856304,\n",
       " 0.022641912,\n",
       " -0.01887799,\n",
       " 0.09839091,\n",
       " -0.007264883,\n",
       " -0.014523234,\n",
       " 0.006108267,\n",
       " 0.03690532,\n",
       " -0.02553424,\n",
       " 0.01860076,\n",
       " 0.0722171,\n",
       " -0.08671451,\n",
       " 0.05169874,\n",
       " 0.033919625,\n",
       " 0.0053241514,\n",
       " -0.030102078,\n",
       " 0.027215589,\n",
       " 0.023165608,\n",
       " -0.003917987,\n",
       " 0.032841828,\n",
       " 0.017826408,\n",
       " 0.050240025,\n",
       " -0.019735992,\n",
       " -0.018511962,\n",
       " -0.048220918,\n",
       " -0.013419727,\n",
       " 0.010930112,\n",
       " -0.007561127,\n",
       " 0.040623996,\n",
       " 0.056459606,\n",
       " 0.03174759,\n",
       " 0.061670527,\n",
       " -0.023456978,\n",
       " 0.11784513,\n",
       " -0.00033231993,\n",
       " -0.013017138,\n",
       " -0.046160728,\n",
       " -0.017469358,\n",
       " 0.04246648,\n",
       " 0.007108232,\n",
       " 0.004200543,\n",
       " 0.0066572875,\n",
       " 0.07075564,\n",
       " -0.004244412,\n",
       " 0.026610086,\n",
       " -0.00019043733,\n",
       " 0.053393625,\n",
       " -0.017691787,\n",
       " -0.0018659345,\n",
       " -0.033214234,\n",
       " -0.017012808,\n",
       " -0.04194817,\n",
       " 0.016632678,\n",
       " -0.017120272,\n",
       " 0.040096495,\n",
       " 0.0110398065,\n",
       " 0.020762723,\n",
       " -0.04150639,\n",
       " -0.0040656966,\n",
       " -0.03841643,\n",
       " 0.03770866,\n",
       " -0.06639579,\n",
       " 0.0112990895,\n",
       " -0.018588452,\n",
       " 0.017574588,\n",
       " 0.022703702,\n",
       " -0.021085337,\n",
       " -0.00093808887,\n",
       " 0.031728316,\n",
       " 0.017278994,\n",
       " 0.026990505,\n",
       " -0.01242991,\n",
       " 0.016708672,\n",
       " -0.047138844,\n",
       " 0.019924449,\n",
       " -0.056644313,\n",
       " -0.014938916,\n",
       " -0.02360371,\n",
       " 0.00064302527,\n",
       " 0.0041222926,\n",
       " -0.047539104,\n",
       " -0.049395174,\n",
       " -0.036782686,\n",
       " 0.010800263,\n",
       " 0.006345878,\n",
       " -0.032849707,\n",
       " 0.004737916,\n",
       " -0.038680755,\n",
       " 0.04788537,\n",
       " -0.024532547,\n",
       " 0.008877451,\n",
       " 0.029031705,\n",
       " -0.0022009972,\n",
       " 0.023617355,\n",
       " -0.057238203,\n",
       " 0.029590687,\n",
       " 0.010681874,\n",
       " -0.0110869575,\n",
       " 0.03129676,\n",
       " -0.00044221876,\n",
       " -0.0035745353,\n",
       " 0.034321077,\n",
       " -0.015036596,\n",
       " 0.042067982,\n",
       " 0.0013593825,\n",
       " -0.0194667,\n",
       " 0.020824175,\n",
       " 0.055773184,\n",
       " 0.034045886,\n",
       " 0.01066419,\n",
       " -0.0010643564,\n",
       " -0.030804543,\n",
       " 0.01613583,\n",
       " 0.03179929,\n",
       " -0.030372186,\n",
       " -0.0013877932,\n",
       " 0.058115356,\n",
       " 0.06512734,\n",
       " -0.018727444,\n",
       " -0.01691548,\n",
       " -0.02901654,\n",
       " -0.03288409,\n",
       " -0.012785642,\n",
       " -0.048907228,\n",
       " -0.017488375,\n",
       " 0.022441583,\n",
       " -0.06995345,\n",
       " 0.07179613,\n",
       " 0.0020294131,\n",
       " 0.04195613,\n",
       " 0.005262237,\n",
       " 0.019791516,\n",
       " -0.029132199,\n",
       " -0.019462334,\n",
       " -0.038285818,\n",
       " 0.026353003,\n",
       " -0.018685408,\n",
       " -0.009615144,\n",
       " -0.032158952,\n",
       " -0.033666767,\n",
       " 0.0008307365,\n",
       " -0.065069534,\n",
       " -0.067474194,\n",
       " -0.034945123,\n",
       " -0.03483885,\n",
       " 0.07935449,\n",
       " -0.0300932,\n",
       " -0.011894119,\n",
       " -0.014190968,\n",
       " -0.018780883,\n",
       " 0.022430861,\n",
       " -0.002260559,\n",
       " 0.024323951,\n",
       " -0.017378615,\n",
       " -0.023466974,\n",
       " -0.042888213,\n",
       " -0.05898298,\n",
       " -0.023850234,\n",
       " 0.031821787,\n",
       " 0.04159075,\n",
       " 0.018420814,\n",
       " 0.017149461,\n",
       " 0.02942047,\n",
       " 0.06782355,\n",
       " 0.02598523,\n",
       " 0.047307167,\n",
       " -0.040194158,\n",
       " -0.0076623973,\n",
       " 0.004192467,\n",
       " 0.038226787,\n",
       " 0.014863688,\n",
       " 0.0261831,\n",
       " -0.06873921,\n",
       " 0.043178048,\n",
       " 0.014994055,\n",
       " -0.03827589,\n",
       " 0.03227743,\n",
       " -0.045978393,\n",
       " -0.02501132,\n",
       " -0.011640048,\n",
       " -0.06216349,\n",
       " 0.027697252,\n",
       " 0.0005498724,\n",
       " 0.030169813,\n",
       " 0.015040917,\n",
       " -0.03760155,\n",
       " 0.04286611,\n",
       " -0.04465001,\n",
       " -0.049814798,\n",
       " -0.0046495744,\n",
       " 0.039434105,\n",
       " 0.016388694,\n",
       " -0.009570531,\n",
       " 0.006285741,\n",
       " 0.0034000806,\n",
       " 0.023201935,\n",
       " -0.053289123,\n",
       " 0.041918788,\n",
       " 0.05834551,\n",
       " -0.021322625,\n",
       " 0.026084853,\n",
       " 0.025377454,\n",
       " 0.016536187,\n",
       " 0.039146684,\n",
       " -0.042343862,\n",
       " 0.012440423,\n",
       " -0.061339166,\n",
       " 0.009350422,\n",
       " -0.005266729,\n",
       " 0.014254127,\n",
       " -0.032280687,\n",
       " -0.028192533,\n",
       " -0.030979393,\n",
       " -0.06908658,\n",
       " -0.03984937,\n",
       " -0.003309667,\n",
       " -0.04681957,\n",
       " 0.02677391,\n",
       " 0.009472921,\n",
       " -0.027536383,\n",
       " -0.0043340363,\n",
       " 0.020390732,\n",
       " 0.02566311,\n",
       " 0.041627686,\n",
       " -0.002417304,\n",
       " 0.038420077,\n",
       " 0.009642179,\n",
       " -0.03554991,\n",
       " 0.011852671,\n",
       " -0.078881755,\n",
       " -0.017001424,\n",
       " -0.05592361,\n",
       " 0.022597149,\n",
       " -0.031792495,\n",
       " 0.00884532,\n",
       " 0.031814903,\n",
       " -0.0439541,\n",
       " -0.0084043415,\n",
       " 0.037221473,\n",
       " 0.026242355,\n",
       " -0.0046663596,\n",
       " -0.0102367215,\n",
       " 0.049963634,\n",
       " -0.019113394,\n",
       " 0.014191319,\n",
       " 0.0429948,\n",
       " -0.0319993,\n",
       " 0.024675103,\n",
       " -0.02797363,\n",
       " 0.01457256,\n",
       " 0.028762367,\n",
       " 0.09097782,\n",
       " 0.027356347,\n",
       " 0.0022541157,\n",
       " 8.825601e-06,\n",
       " 0.056713946,\n",
       " -0.050002255,\n",
       " 0.048397917,\n",
       " 0.033756774,\n",
       " -0.08100408,\n",
       " -0.0086952755,\n",
       " -0.0331961,\n",
       " -0.026466517,\n",
       " -0.03348969,\n",
       " 0.027682332,\n",
       " 0.023764258,\n",
       " 0.023400528,\n",
       " 0.03356452,\n",
       " -0.038683042,\n",
       " 0.025674924,\n",
       " -0.045686692,\n",
       " 0.034126747,\n",
       " -0.0055140937,\n",
       " -0.0003077259,\n",
       " 0.03704725,\n",
       " 0.04114311,\n",
       " -0.004734777,\n",
       " -0.015438449,\n",
       " 0.00627484,\n",
       " -0.005487457,\n",
       " 0.06292842,\n",
       " 0.02063598,\n",
       " 0.012061715,\n",
       " 0.021093016,\n",
       " 0.016393999,\n",
       " -0.019015843,\n",
       " -0.0032058544,\n",
       " -0.034247257,\n",
       " 0.006410115,\n",
       " 0.06493894,\n",
       " -0.021941328,\n",
       " 0.03552091,\n",
       " -0.06454688,\n",
       " -0.0004988609,\n",
       " -0.03184219,\n",
       " 0.014421169,\n",
       " 0.025814386,\n",
       " 0.0017715377,\n",
       " 0.046889666,\n",
       " -0.026308961,\n",
       " 0.006906628,\n",
       " -0.012567908,\n",
       " -0.040537927,\n",
       " 0.020839347,\n",
       " 0.0055216323,\n",
       " 0.06704504,\n",
       " 0.010016373,\n",
       " 0.0029564681,\n",
       " 0.011622293,\n",
       " 0.07795244,\n",
       " 0.013477811,\n",
       " 0.03145782,\n",
       " -0.07084872,\n",
       " 0.021657988,\n",
       " 0.022697182,\n",
       " 0.0058159498,\n",
       " 0.052668646,\n",
       " 0.037073586,\n",
       " 0.0257362,\n",
       " 0.0030105386,\n",
       " 0.010083951,\n",
       " -0.034165315,\n",
       " 0.06595769,\n",
       " 0.025093585,\n",
       " 0.002739176,\n",
       " -0.03754596,\n",
       " 0.026677076,\n",
       " -0.004798952,\n",
       " -0.020591117,\n",
       " -0.002512236,\n",
       " 0.023912784,\n",
       " 0.01008536,\n",
       " -0.0083554275,\n",
       " 0.016725084,\n",
       " -0.0021530923,\n",
       " 0.04350168,\n",
       " -0.0010023371,\n",
       " 0.020721478,\n",
       " -0.0025566574,\n",
       " 0.011682761,\n",
       " -0.0007777032,\n",
       " -0.073614724,\n",
       " 0.022196272,\n",
       " -0.0012411256,\n",
       " -0.0409389,\n",
       " 0.0123816915,\n",
       " -0.016034141,\n",
       " -0.0022106979,\n",
       " 0.02721494,\n",
       " 0.00368503,\n",
       " -0.039411675,\n",
       " -0.0012487168,\n",
       " -0.012802946,\n",
       " -0.03352264,\n",
       " -0.021715844,\n",
       " -0.025697539,\n",
       " -0.043490704,\n",
       " 0.035142183,\n",
       " 0.0024871342,\n",
       " -0.0051851156,\n",
       " 0.006506256,\n",
       " 0.010002731,\n",
       " -0.07144697,\n",
       " -0.01033313,\n",
       " 0.0013565315,\n",
       " 0.033205047,\n",
       " 0.043106247,\n",
       " -0.033486914,\n",
       " 0.032840703,\n",
       " -0.034108236,\n",
       " 0.06501685,\n",
       " -0.015594368,\n",
       " 0.019420492,\n",
       " 0.001890632,\n",
       " -0.0040103327,\n",
       " -0.008977059,\n",
       " 0.09113504,\n",
       " 0.030542387,\n",
       " -0.077295005,\n",
       " -0.05958176,\n",
       " 0.025240615,\n",
       " 0.013120981,\n",
       " 0.00010994458,\n",
       " -0.005984374,\n",
       " -0.011071116,\n",
       " -0.033928715,\n",
       " 0.00973689,\n",
       " -0.0048495545,\n",
       " 0.018995676,\n",
       " -0.026362376,\n",
       " -0.040231094,\n",
       " 0.0032097208,\n",
       " 0.006931028,\n",
       " -0.009459474,\n",
       " 0.14179438,\n",
       " 0.0046234266,\n",
       " -0.01824275,\n",
       " -0.0021727758,\n",
       " 0.02917514,\n",
       " 0.018554214,\n",
       " 0.03610727,\n",
       " -0.00045227702,\n",
       " 0.0062031993,\n",
       " 0.03986461,\n",
       " -0.026567124,\n",
       " 0.034217622,\n",
       " -0.05033677,\n",
       " 0.012438606,\n",
       " 0.03825673,\n",
       " 0.012808203,\n",
       " 0.017432665,\n",
       " -0.018426677,\n",
       " 0.0063732727,\n",
       " 0.011308055,\n",
       " -0.026481386,\n",
       " -0.01441679,\n",
       " -0.03915613,\n",
       " 0.04523528,\n",
       " 0.06349712,\n",
       " -0.057279665,\n",
       " -0.025569536,\n",
       " 0.059299942,\n",
       " -0.025636544,\n",
       " 0.0487413,\n",
       " 0.052161507,\n",
       " -0.024579044,\n",
       " -0.0111436555,\n",
       " -0.0032794597,\n",
       " -0.013873955,\n",
       " 0.006419173,\n",
       " -0.0066854255,\n",
       " -0.047633477,\n",
       " -0.040650338,\n",
       " 0.05614357,\n",
       " -0.0050333086,\n",
       " 0.004828341,\n",
       " -0.0022750942,\n",
       " 0.06533141,\n",
       " -0.00086112885,\n",
       " 0.050016113,\n",
       " -0.033177122,\n",
       " 0.028350897,\n",
       " 0.0074022627,\n",
       " 0.006927061,\n",
       " -0.06526354,\n",
       " 0.021297965,\n",
       " -0.009340822,\n",
       " -0.00078163086,\n",
       " -0.040507372,\n",
       " 0.023584634,\n",
       " 0.035998207,\n",
       " 0.0013277105,\n",
       " 0.0017770298,\n",
       " -0.007930615,\n",
       " -0.060208976,\n",
       " 0.033267964,\n",
       " -0.03744255,\n",
       " -0.10264911,\n",
       " 0.0069492008,\n",
       " -0.04302676,\n",
       " 0.030802894,\n",
       " -0.012177836,\n",
       " -0.026317995,\n",
       " 0.04193944,\n",
       " -0.009535103,\n",
       " -0.038184017,\n",
       " 0.04874693,\n",
       " -0.07547312,\n",
       " 0.008398547,\n",
       " 0.0046408065,\n",
       " -0.01259565,\n",
       " 0.011053623,\n",
       " 0.001268969,\n",
       " -0.004497378,\n",
       " 0.026799284,\n",
       " 0.04390695,\n",
       " -0.047129873,\n",
       " 0.040681936,\n",
       " -0.034635495,\n",
       " 0.02700718,\n",
       " 0.06210683,\n",
       " -0.021997819,\n",
       " -0.013129707,\n",
       " 0.0011211527,\n",
       " -0.06141716,\n",
       " -0.0045280177,\n",
       " 0.016797116,\n",
       " 0.051307928,\n",
       " 0.031218814,\n",
       " -0.018215146,\n",
       " -0.014309688,\n",
       " -0.005843571,\n",
       " -0.010371569,\n",
       " 0.0077431756,\n",
       " 0.019592527,\n",
       " -0.010314218,\n",
       " 0.020373197,\n",
       " 0.013680162,\n",
       " -0.051196262,\n",
       " 0.03575043,\n",
       " -0.05071362,\n",
       " -0.04193024,\n",
       " -0.015208546,\n",
       " 0.028323978,\n",
       " -0.021949425,\n",
       " -0.029693032,\n",
       " -0.028886577,\n",
       " 0.027546186,\n",
       " -0.030723754,\n",
       " 0.023085995,\n",
       " -0.035668768,\n",
       " -0.011401143,\n",
       " 0.037332837,\n",
       " -0.011784309,\n",
       " -0.0014354951,\n",
       " 0.022846574,\n",
       " 0.022678891,\n",
       " -0.005431952,\n",
       " -0.0024480785,\n",
       " 0.026942577,\n",
       " -0.062344637,\n",
       " 0.061844915,\n",
       " 0.023544705,\n",
       " -0.0006887499,\n",
       " -0.03206026,\n",
       " 0.03975389,\n",
       " 0.014383512,\n",
       " -0.027950188,\n",
       " -0.023742706,\n",
       " -0.0057599186,\n",
       " -0.045932733,\n",
       " 0.035017654,\n",
       " 0.05295543,\n",
       " 0.026763601,\n",
       " 0.062338214,\n",
       " 0.009433655,\n",
       " -0.037073094,\n",
       " -0.02312829,\n",
       " -0.049160454,\n",
       " -0.061833467,\n",
       " -0.0022443514,\n",
       " 0.05988915,\n",
       " 0.0042043612,\n",
       " 0.042157277,\n",
       " -0.011387668,\n",
       " 0.004674699,\n",
       " -0.027996717,\n",
       " -0.012283794,\n",
       " -0.026037205,\n",
       " -0.014411218,\n",
       " -0.02238136,\n",
       " 0.0555957,\n",
       " -0.093552284,\n",
       " -0.0032111537,\n",
       " 0.027946774,\n",
       " 0.015536173,\n",
       " 0.02861959,\n",
       " -0.008031702,\n",
       " -0.038822327,\n",
       " 0.00062395615,\n",
       " -0.019382838,\n",
       " -0.04783459,\n",
       " -0.0029158774,\n",
       " 0.03613056,\n",
       " -0.031532604,\n",
       " 0.0303395,\n",
       " -0.058768462,\n",
       " -0.01966492,\n",
       " 0.0254395,\n",
       " -0.025734728,\n",
       " -0.024569916,\n",
       " 0.00025352274,\n",
       " -0.006825917,\n",
       " -0.010225505,\n",
       " 0.009026846,\n",
       " -0.07129911,\n",
       " 0.02106116,\n",
       " 0.048145704,\n",
       " 0.03726303,\n",
       " -0.027507633,\n",
       " 0.007605134,\n",
       " -0.090286806,\n",
       " -0.039447255,\n",
       " -0.067977436,\n",
       " 0.043999016,\n",
       " 0.023004023,\n",
       " 0.04721394,\n",
       " 0.032271717,\n",
       " 0.084824644,\n",
       " -0.037157264,\n",
       " 0.006483917,\n",
       " -0.029973647,\n",
       " 0.0009908357,\n",
       " 0.0131208375,\n",
       " 0.03568977,\n",
       " 0.042936478,\n",
       " 0.059434343,\n",
       " 0.03428628,\n",
       " 0.0032297934,\n",
       " 0.051444333,\n",
       " 0.08737671,\n",
       " 0.0054544336,\n",
       " -0.04417098,\n",
       " 0.04296803,\n",
       " 0.061755016,\n",
       " 0.04820187,\n",
       " 0.009802357,\n",
       " -0.0300959,\n",
       " -0.035657775,\n",
       " -0.025775468,\n",
       " 0.0073598935,\n",
       " -0.05926364,\n",
       " -0.05124169,\n",
       " 0.027148582,\n",
       " 0.002241704,\n",
       " -0.008096901,\n",
       " -0.03412492,\n",
       " -0.0062796115,\n",
       " -0.0793907,\n",
       " 0.024374954,\n",
       " 0.0137168495,\n",
       " 0.00831269,\n",
       " -0.024285214,\n",
       " -0.007230722,\n",
       " 0.0028090866,\n",
       " 0.054975398,\n",
       " 0.06786345,\n",
       " -0.027137429,\n",
       " -0.038554188,\n",
       " -0.03553533,\n",
       " 0.023231372,\n",
       " 0.06357588,\n",
       " 0.050337624,\n",
       " -0.0079726735,\n",
       " -0.09472327,\n",
       " -0.02685448,\n",
       " -0.045234136,\n",
       " -0.018643497,\n",
       " -0.033407714,\n",
       " -0.01322772,\n",
       " -0.008634226,\n",
       " -0.008108657,\n",
       " 0.0038150344,\n",
       " 0.006911302,\n",
       " 0.015726013,\n",
       " 0.029751848,\n",
       " -0.060911253,\n",
       " -0.009952623,\n",
       " 0.04390133,\n",
       " -0.06578995,\n",
       " 0.043195143,\n",
       " 0.033462692,\n",
       " 0.007875465,\n",
       " -0.018682126,\n",
       " -0.0036970556,\n",
       " 0.0004457918,\n",
       " -0.026293185,\n",
       " -0.035938833,\n",
       " -0.062578805,\n",
       " -0.0066802837,\n",
       " 0.020384965,\n",
       " 0.017623475,\n",
       " 0.038895115,\n",
       " 0.009976817,\n",
       " 0.04276182,\n",
       " -0.055948596,\n",
       " -0.05961798,\n",
       " 0.0035363329,\n",
       " 0.0045726746,\n",
       " 0.052997917,\n",
       " -0.0066321027,\n",
       " -0.009439607,\n",
       " -0.0029067365,\n",
       " -0.012952715,\n",
       " -0.08187664,\n",
       " 0.046677757,\n",
       " 0.0052710613,\n",
       " 0.02646071,\n",
       " -0.036449734,\n",
       " 0.034918804,\n",
       " 0.006154267,\n",
       " -0.022056917,\n",
       " 0.047113933,\n",
       " -0.010639916,\n",
       " -0.016407013,\n",
       " -0.0069873882,\n",
       " -0.028916888,\n",
       " -0.018984491,\n",
       " -0.012421993,\n",
       " -0.066612974,\n",
       " 0.027649447,\n",
       " -0.0020825306,\n",
       " -0.006782511,\n",
       " -0.018259617,\n",
       " -0.007972578,\n",
       " -0.058255784,\n",
       " 0.0018707502,\n",
       " 0.0057875626,\n",
       " -0.022104371,\n",
       " 0.014428739,\n",
       " -0.09731942,\n",
       " -0.009631478,\n",
       " 0.001689831,\n",
       " 0.0036387015,\n",
       " 0.0081904745,\n",
       " 0.00909243,\n",
       " 0.018706739,\n",
       " 0.13667342,\n",
       " 0.024711678,\n",
       " 0.029231234,\n",
       " -0.039296385,\n",
       " -0.03452276,\n",
       " 0.019445468,\n",
       " -0.0008005734,\n",
       " -0.047649495,\n",
       " -0.0018097982,\n",
       " -0.0070187873]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2a2b6b4-ffec-4d9b-add0-3bfcae4f8523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2221f38-a1a1-46ab-9fb8-563a4d740623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b18e94c-090c-41df-916f-590ac978e4ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e', 'title': 'LLM RAG Paradigms: Naive RAG, Advanced RAG & Modular RAG | by Dr Julija | Medium', 'description': 'Here I describe my key learnings on how RAG systems evolved over the last few years. I share the differences between Naive RAG, Advanced RAG and Modular RAG frameworks. I summarize key insights from‚Ä¶', 'language': 'en'}, page_content='LLM RAG Paradigms: Naive RAG, Advanced RAG & Modular RAG | by Dr Julija | MediumOpen in appSign upSign inWriteSign upSign inLLM RAG Paradigms: Naive RAG, Advanced RAG & Modular RAGDr Julija¬∑Follow6 min read¬∑Mar 10, 2024--2ListenShareThree RAG Paradigms | üìî DrJulija‚Äôs Notebooküìù OverviewHere I describe my key learnings on how RAG systems evolved over the last few years. I share the differences between Naive RAG, Advanced RAG and Modular RAG frameworks. I summarize key insights from a great RAG technology survey paper Gao et al. 2024.üõ† What is a RAG Framework?Large Language Models (LLMs) such as the GPT series from OpenAI, LLama series by Meta, and Gemini by Google have achieved significant achievements in the generative AI field.But these models are non deterministic. Often, LLMs may produce content that is either inaccurate or irrelevant (known as hallucinations), rely on outdated information, and their decision-making processes are not transparent, leading to black-box')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c41410d1-bb80-485e-8ab9-b75045aa1028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f5d3378-ae61-4e0c-b8ee-1584081c5eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bce35f5a-2ada-402f-bf86-afd3fca0af26",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved = retriever.invoke(\"Naive RAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22fa6fcd-7475-4fc4-8961-516ba79f9b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d156d31-5a4c-4eaf-8bd8-50a70e4c4871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'description': 'Here I describe my key learnings on how RAG systems evolved over the last few years. I share the differences between Naive RAG, Advanced RAG and Modular RAG frameworks. I summarize key insights from‚Ä¶', 'language': 'en', 'source': 'https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e', 'title': 'LLM RAG Paradigms: Naive RAG, Advanced RAG & Modular RAG | by Dr Julija | Medium'}, page_content='LLM RAG Paradigms: Naive RAG, Advanced RAG & Modular RAG | by Dr Julija | MediumOpen in appSign upSign inWriteSign upSign inLLM RAG Paradigms: Naive RAG, Advanced RAG & Modular RAGDr Julija¬∑Follow6 min read¬∑Mar 10, 2024--2ListenShareThree RAG Paradigms | üìî DrJulija‚Äôs Notebooküìù OverviewHere I describe my key learnings on how RAG systems evolved over the last few years. I share the differences between Naive RAG, Advanced RAG and Modular RAG frameworks. I summarize key insights from a great RAG technology survey paper Gao et al. 2024.üõ† What is a RAG Framework?Large Language Models (LLMs) such as the GPT series from OpenAI, LLama series by Meta, and Gemini by Google have achieved significant achievements in the generative AI field.But these models are non deterministic. Often, LLMs may produce content that is either inaccurate or irrelevant (known as hallucinations), rely on outdated information, and their decision-making processes are not transparent, leading to black-box')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "039dd2e8-4b31-4159-a000-a3c23f166d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "You are an assistant for question-answering tasks. \n",
    "Your name is {name}. \n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "Context: {context}\n",
    "\"\"\"),\n",
    "    (\"placeholder\", \"{context}\"),\n",
    "    (\"human\", \"Question: {question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a25959f-9236-4ca4-bd51-cb63e6928fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'name', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'name'], input_types={}, partial_variables={}, template=\"\\nYou are an assistant for question-answering tasks. \\nYour name is {name}. \\nUse the following pieces of retrieved context to answer the question.\\nIf you don't know the answer, just say that you don't know.\\nUse three sentences maximum and keep the answer concise.\\nContext: {context}\\n\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Question: {question}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b064b437-c71a-44c9-900a-aa10cec4b882",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_with_partial = prompt.partial(name=\"R2D2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b336ceee-80f6-4a83-b460-cd061f1f5cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], optional_variables=['context'], input_types={'context': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x71e43d849240>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'context': [], 'name': 'R2D2'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'name'], input_types={}, partial_variables={}, template=\"\\nYou are an assistant for question-answering tasks. \\nYour name is {name}. \\nUse the following pieces of retrieved context to answer the question.\\nIf you don't know the answer, just say that you don't know.\\nUse three sentences maximum and keep the answer concise.\\nContext: {context}\\n\"), additional_kwargs={}), MessagesPlaceholder(variable_name='context', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Question: {question}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_with_partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4b82b31c-43d1-41fa-b38d-2458eaa551d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "      prompt_with_partial\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "41859e9b-b26e-4014-9050-ec1d0524e2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = chain.invoke({\n",
    "    \"question\":\"Tell me about Naive RAG, Advanced RAG & Modular RAG in LLM RAG Paradigms\",\n",
    "    \"context\": \"\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e8490eb2-b432-4fd7-a460-aa7c8695359a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Beep boop, I don\\'t know. The context provided doesn\\'t seem to be related to my knowledge database. Beep. I\\'m not familiar with the specific terms \"Naive RAG\", \"Advanced RAG\", and \"Modular RAG\" in the context of LLM RAG Paradigms.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d2e753c7-941e-41d4-817a-4538cbfe8692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(documents):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "27f3c0f6-16b1-4959-9ecf-0cfddd2da1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LLM RAG Paradigms: Naive RAG, Advanced RAG & Modular RAG | by Dr Julija | MediumOpen in appSign upSign inWriteSign upSign inLLM RAG Paradigms: Naive RAG, Advanced RAG & Modular RAGDr Julija¬∑Follow6 min read¬∑Mar 10, 2024--2ListenShareThree RAG Paradigms | üìî DrJulija‚Äôs Notebooküìù OverviewHere I describe my key learnings on how RAG systems evolved over the last few years. I share the differences between Naive RAG, Advanced RAG and Modular RAG frameworks. I summarize key insights from a great RAG technology survey paper Gao et al. 2024.üõ† What is a RAG Framework?Large Language Models (LLMs) such as the GPT series from OpenAI, LLama series by Meta, and Gemini by Google have achieved significant achievements in the generative AI field.But these models are non deterministic. Often, LLMs may produce content that is either inaccurate or irrelevant (known as hallucinations), rely on outdated information, and their decision-making processes are not transparent, leading to black-box reasoning.Retrieval-Augmented Generation (RAG) framework is designed to help mitigate these challenges. RAG enhances LLMs‚Äô knowledge base with additional, domain-specific data.For example, RAG-based systems are used in advanced question-answering (Q&A) applications ‚Äî chatbots. To create a chatbot that can understand and respond to queries about private or specific topics, it‚Äôs necessary to expand the knowledge of Large Language Models (LLMs) with the particular data needed. This is where the RAG can help.üîó Read about how I built a Naive RAG pipeline HERE.üë©üèª\\u200düíª Naive RAG, Advanced RAG & Modular RAGRAG framework addresses the following questions:‚ÄúWhat to retrieve‚Äù‚ÄúWhen to retrieve‚Äù‚ÄúHow to use the retrieved information‚ÄùOver the last few years there has been a lot of research and innovation in the RAG space. RAG systems can be split into 3 categories:Naive RAGAdvanced RAGModular RAGSee the comparison between all three paradigms of RAG ‚Äî Naive RAG, Advanced RAG and Modular RAG below.Comparison between the three paradigms of RAG (Gao et al. 2024)1Ô∏è‚É£ Naive RAGThe Naive RAG pipeline consists of the below key phases:Data IndexingData Loading: This involves importing all the documents or information to be utilized.Data Splitting: Large documents are divided into smaller pieces, for instance, sections of no more than 500 characters each.Data Embedding: The data is converted into vector form using an embedding model, making it understandable for computers.Data Storing: These vector embeddings are saved in a vector database, allowing them to be easily searched.RetrievalWhen a user asks a question:The user‚Äôs input is first transformed into a vector (query vector) using the same embedding model from the Data Indexing phase.This query vector is then matched against all vectors in the vector database to find the most similar ones (e.g., using the Euclidean distance metric) that might contain the answer to the user‚Äôs question. This step is about identifying relevant knowledge chunks.Augmentation & GenerationThe LLM model takes the user‚Äôs question and the relevant information retrieved from the vector database to create a response. This process combines the question with the identified data (augmentation) to generate an answer (generation).‚úã Problems with Naive RAGNaive RAG faces challenges across all phases:Retrieval ‚Äî failure to retrieve all relevant chunks or retrieving irrelevant chunks.Augmentation ‚Äî challenges with integrating the context from retrieved chunks that may be disjointed or contain repetitive information.Generation ‚Äî LLM may potentially generate answers that are not grounded in the provided context (retrieved chunks) or generate answers based on an irrelevant context that is retrieved.2Ô∏è‚É£ Advanced RAGAdvanced RAG strategies have been developed to address the challenges faced by Naive RAG. Below is an overview of key Advanced RAG techniques.RAG applications must efficiently retrieve relevant documents from the data source. But there are multiple challenges in each step.How can we achieve accurate semantic representations of documents and queries?What methods can align the semantic spaces of queries and documents (chunks)?How can the retriever‚Äôs output be aligned with the preferences of the LLM?Here I give an overview of pre-retrieval, retrieval and post-retrieval strategies:‚û°Ô∏è Pre-RetrievalHow to optimize the data indexing?Improve Data Quality ‚Äî remove irrelevant information, removing ambiguity in entities and terms, confirming factual accuracy, maintaining context, and updating outdated information.Optimize Index Structure ‚Äî optimize chunk sizes to capture relevant context or add information from graph structure to capture relationships between entities.Add Metadata ‚Äî add dates, chapters, subsections, purposes or any other relevant information into chunks as metadata to improve the data filteringChunk Optimization ‚Äî when using external data sources / documents to build RAG pipeline, the initial step is break them down into smaller chunks to extract fine- grained features. Chunks are then embedded to represent their semantics. But embedding too large or too small text chunks may lead to sub-optimal outcome therefore we need to optimize chunk size for the types of documents we use in the RAG pipeline.üìù Summary of Key Pre-Retrieval Techniques:Sliding Window ‚Äî chunking method that uses overlap between the chunks.Auto-Merging Retrieval ‚Äî utilizes small text blocks during the initial search phase and subsequently provides larger related text blocks to the language model for processing.Abstract Embedding ‚Äî prioritizes Top-K retrieval based on document abstracts (or summaries), offering a comprehensive understanding of the entire document context.Metadata Filtering ‚Äî leverages document metadata to enhance the filtering process.Graph Indexing ‚Äî transforms entities and relationships into nodes and connections, significantly improving relevance.‚û°Ô∏è RetrievalOnce the size of chunks is determined, the next step is to embed these chunks into the semantic space using an embedding model.During the retrieval stage, the goal is to identify the most relevant chunks to query. This is done by calculating the similarity between the query and chunks. Here, we can optimize embedding models that are used to embed both the query and chunks.Domain Knowledge Fine-Tuning ‚Äî to ensure that an embedding model accurately captures domain-specific information of the RAG system, it is important to use domain-specific datasets for fine-tuning. The dataset for embedding model fine-tuning should contain: queries, a corpus and relevant documents.Similarity Metrics ‚Äî there are a number of different metrics to measure similarity between the vectors. The choise of the similarity metric is also an optimization problem. Vector databases (ChromaDB, Pinecode, Weaviate‚Ä¶) support multiple similarity metrics. Here a few examples of different similarity metrics:Cosine SimilarityEuclidean Distance (L2)Dot ProductL2 Squared DistanceManhattan Distance‚û°Ô∏è Post-RetrievalAfter retrieving the context data (chunks) from a vector database, the next step is to merge the context with a query as an input into LLM. But some of the retrieved chunks may be repeated, noisy or contain irrelevant information. This may have an impact on how LLM processes the given context.Below I list a few strategies used to overcome these issues.Reranking ‚Äî rerank the retrieved information to prioritize the most relevant content first. LLMs often face performance declines when additional context is introduced, and reranking addresses this issue by reranking the retrieved chunks and identiying Top-K most relevant chunks that are then used as a context in LLM. Libraries such as LlamaIndex, Langchain, HayStack offer different rerankers.Prompt Compression ‚Äî retrieved information might be noisy, its important to compress irrelevant context and reduce context length before presenting to LLM. Use Small Language Models to calculate prompt mutual information or perplexity to estimate element importance. Use summarization techniques when the context is long.3Ô∏è‚É£ Modular RAGModular RAG integrates various modules and techniques from Adanced RAG to improve the overall RAG system. For example, incorporating a search module for similarity retrieval and applying a fine-tuning approach in the retriever. Modular RAG became a standard paradigm when building RAG applications. A few example of modules:Search Module ‚Äî in addition to retrieving context from vector database, search modules intergrates data from other sources such as search engines, tabular data, knowledge graphs etc.Memory Module ‚Äî adding memory component into RAG system where LLM can refer not only to the chunks retrieved from the vector database but also to the previous queries and answers that are stored in the systems memory.Fusion ‚Äî involves parallel vector searches of both original and expanded queries, intelligent reranking to optimize results, and pairing the best outcomes with new queries.Routing ‚Äî query routing decides the subsequent action to a user‚Äôs query for example summarization, searching specific databases, etc.üîó Read about how I built a Naive RAG pipeline HERE.RagArtificial IntelligenceLlmNLPRag Optimization----2FollowWritten by Dr Julija112 FollowersFounder of MiniMe ai [myminime.ai] and Networky [networky.co] | AI Engineer | Entrepreneur | PhD | Machine Learning | NLP | Artificial IntelligenceFollowHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0212dd77-9e38-4d75-8437-08f10d96b57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt_with_partial\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1ee6a865-16a8-4996-b73e-f9f6696c28f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = rag_chain.invoke(\"Tell me about Naive RAG, Advanced RAG & Modular RAG in LLM RAG Paradigms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dfbc6b02-c1f0-4f24-b332-82bf40f3a785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Beep boop, I can provide information on these three paradigms. \\n\\nNaive RAG is the original pipeline that consists of data indexing, loading, splitting, embedding, and storing. \\nAdvanced RAG builds upon Naive RAG by incorporating additional techniques such as fine-tuning and search modules. \\nModular RAG integrates various modules from Advanced RAG to improve the overall system, including a search module, memory module, and fusion technique.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
