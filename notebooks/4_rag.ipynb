{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1_rag_basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade python-dotenv langchain langchain-community langchain-ollama langchain-chroma chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_relevante_docs(docs):\n",
    "    # Display the relevant results with metadata\n",
    "    print(\"\\n--- Relevant Documents ---\")\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        print(f\"Document {i}:\\n{doc.page_content}\\n\")\n",
    "        if doc.metadata:\n",
    "            print(f\"Source: {doc.metadata.get('source', 'Unknown')}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing the text file and the persistent directory\n",
    "current_dir = os.path.dirname(os.getcwd())\n",
    "file_path = os.path.join(current_dir, \"notebooks\", \"documents\", \"langchain_demo.txt\")\n",
    "persistent_directory = os.path.join(current_dir, \"notebooks\", \"chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text content from the file\n",
    "loader = TextLoader(file_path)\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the document into chunks\n",
    "text_splitter = CharacterTextSplitter(separator=\"\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,)\n",
    "docs_split = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Document Chunks Information ---\n",
      "Number of document chunks: 4\n",
      "Sample chunk:\n",
      "LangChain Documentation: https://python.langchain.com/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display information about the split documents\n",
    "print(\"\\n--- Document Chunks Information ---\")\n",
    "print(f\"Number of document chunks: {len(docs_split)}\")\n",
    "print(f\"Sample chunk:\\n{docs_split[-1].page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Creating embeddings ---\n",
      "\n",
      "--- Finished creating embeddings ---\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings\n",
    "print(\"\\n--- Creating embeddings ---\")\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "print(\"\\n--- Finished creating embeddings ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = embeddings.embed_documents([\"Sample test for embedding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Creating vector store ---\n",
      "\n",
      "--- Finished creating vector store ---\n"
     ]
    }
   ],
   "source": [
    "# Create the vector store and persist it automatically\n",
    "print(\"\\n--- Creating vector store ---\")\n",
    "vectorstore = Chroma.from_documents(documents=docs_split, embedding=embeddings, persist_directory=persistent_directory)\n",
    "print(\"\\n--- Finished creating vector store ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve relevant documents based on the query\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 2},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the user's question\n",
    "query = \"What is LangChain?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_docs = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Relevant Documents ---\n",
      "Document 1:\n",
      "LangChain is a powerful and flexible framework designed to simplify the development of applications that harness the capabilities of large language models (LLMs). It provides a wide range of tools, abstractions, and integrations that help developers build, customize, and optimize applications that leverage LLMs for tasks like text generation, question answering, summarization, chatbots, and more.\n",
      "\n",
      "Source: /home/mrego/Projects/workspace/langchain-notebook/notebooks/documents/langchain_demo.txt\n",
      "\n",
      "Document 2:\n",
      "LangChain Documentation: https://python.langchain.com/\n",
      "\n",
      "Source: /home/mrego/Projects/workspace/langchain-notebook/notebooks/documents/langchain_demo.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the relevant results with metadata\n",
    "print_relevante_docs(relevant_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2_web_scrape_basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\",)\n",
    ")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,)\n",
    "docs_split = text_splitter.split_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Document Chunks Information ---\n",
      "Number of document chunks: 12\n",
      "Sample chunk:\n",
      "are stored in the systems memory.Fusion ‚Äî involves parallel vector searches of both original and expanded queries, intelligent reranking to optimize results, and pairing the best outcomes with new queries.Routing ‚Äî query routing decides the subsequent action to a user‚Äôs query for example summarization, searching specific databases, etc.üîó Read about how I built a Naive RAG pipeline HERE.RagArtificial IntelligenceLlmNLPRag Optimization----2FollowWritten by Dr Julija134 Followers¬∑6 FollowingFounder of MiniMe ai [myminime.ai] and Networky [networky.co] | AI Engineer | Entrepreneur | PhD | Machine Learning | NLP | Artificial IntelligenceFollowResponses (2)See all responsesHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display information about the split documents\n",
    "print(\"\\n--- Document Chunks Information ---\")\n",
    "print(f\"Number of document chunks: {len(docs_split)}\")\n",
    "print(f\"Sample chunk:\\n{docs_split[-1].page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Creating vector store ---\n",
      "\n",
      "--- Finished creating vector store ---\n"
     ]
    }
   ],
   "source": [
    "# Create the vector store and persist it automatically\n",
    "print(\"\\n--- Creating vector store ---\")\n",
    "vectorstore = Chroma.from_documents(documents=docs_split, embedding=embeddings, persist_directory=persistent_directory)\n",
    "print(\"\\n--- Finished creating vector store ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve relevant documents based on the query\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"k\": 3, \"score_threshold\": 0.3},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_docs = retriever.invoke(\"Naive RAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Relevant Documents ---\n",
      "Document 1:\n",
      "LLM RAG Paradigms: Naive RAG, Advanced RAG & Modular RAG | by Dr Julija | MediumOpen in appSign upSign inWriteSign upSign inLLM RAG Paradigms: Naive RAG, Advanced RAG & Modular RAGDr Julija¬∑Follow6 min read¬∑Mar 10, 2024--2ListenShareThree RAG Paradigms | üìî DrJulija‚Äôs Notebooküìù OverviewHere I describe my key learnings on how RAG systems evolved over the last few years. I share the differences between Naive RAG, Advanced RAG and Modular RAG frameworks. I summarize key insights from a great RAG technology survey paper Gao et al. 2024.üõ† What is a RAG Framework?Large Language Models (LLMs) such as the GPT series from OpenAI, LLama series by Meta, and Gemini by Google have achieved significant achievements in the generative AI field.But these models are non deterministic. Often, LLMs may produce content that is either inaccurate or irrelevant (known as hallucinations), rely on outdated information, and their decision-making processes are not transparent, leading to black-box\n",
      "\n",
      "Source: https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\n",
      "\n",
      "Document 2:\n",
      "LLM RAG Paradigms: Naive RAG, Advanced RAG & Modular RAG | by Dr Julija | MediumOpen in appSign upSign inWriteSign upSign inLLM RAG Paradigms: Naive RAG, Advanced RAG & Modular RAGDr Julija¬∑Follow6 min read¬∑Mar 10, 2024--2ListenShareThree RAG Paradigms | üìî DrJulija‚Äôs Notebooküìù OverviewHere I describe my key learnings on how RAG systems evolved over the last few years. I share the differences between Naive RAG, Advanced RAG and Modular RAG frameworks. I summarize key insights from a great RAG technology survey paper Gao et al. 2024.üõ† What is a RAG Framework?Large Language Models (LLMs) such as the GPT series from OpenAI, LLama series by Meta, and Gemini by Google have achieved significant achievements in the generative AI field.But these models are non deterministic. Often, LLMs may produce content that is either inaccurate or irrelevant (known as hallucinations), rely on outdated information, and their decision-making processes are not transparent, leading to black-box\n",
      "\n",
      "Source: https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\n",
      "\n",
      "Document 3:\n",
      "LLM RAG Paradigms: Naive RAG, Advanced RAG & Modular RAG | by Dr Julija | MediumOpen in appSign upSign inWriteSign upSign inLLM RAG Paradigms: Naive RAG, Advanced RAG & Modular RAGDr Julija¬∑Follow6 min read¬∑Mar 10, 2024--2ListenShareThree RAG Paradigms | üìî DrJulija‚Äôs Notebooküìù OverviewHere I describe my key learnings on how RAG systems evolved over the last few years. I share the differences between Naive RAG, Advanced RAG and Modular RAG frameworks. I summarize key insights from a great RAG technology survey paper Gao et al. 2024.üõ† What is a RAG Framework?Large Language Models (LLMs) such as the GPT series from OpenAI, LLama series by Meta, and Gemini by Google have achieved significant achievements in the generative AI field.But these models are non deterministic. Often, LLMs may produce content that is either inaccurate or irrelevant (known as hallucinations), rely on outdated information, and their decision-making processes are not transparent, leading to black-box\n",
      "\n",
      "Source: https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the relevant results with metadata\n",
    "print_relevante_docs(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_docs = retriever.invoke(\"Tell me about Naive RAG, Advanced RAG & Modular RAG in LLM RAG Paradigms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Relevant Documents ---\n",
      "Document 1:\n",
      "LLM RAG Paradigms: Naive RAG, Advanced RAG & Modular RAG | by Dr Julija | MediumOpen in appSign upSign inWriteSign upSign inLLM RAG Paradigms: Naive RAG, Advanced RAG & Modular RAGDr Julija¬∑Follow6 min read¬∑Mar 10, 2024--2ListenShareThree RAG Paradigms | üìî DrJulija‚Äôs Notebooküìù OverviewHere I describe my key learnings on how RAG systems evolved over the last few years. I share the differences between Naive RAG, Advanced RAG and Modular RAG frameworks. I summarize key insights from a great RAG technology survey paper Gao et al. 2024.üõ† What is a RAG Framework?Large Language Models (LLMs) such as the GPT series from OpenAI, LLama series by Meta, and Gemini by Google have achieved significant achievements in the generative AI field.But these models are non deterministic. Often, LLMs may produce content that is either inaccurate or irrelevant (known as hallucinations), rely on outdated information, and their decision-making processes are not transparent, leading to black-box\n",
      "\n",
      "Source: https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\n",
      "\n",
      "Document 2:\n",
      "LLM RAG Paradigms: Naive RAG, Advanced RAG & Modular RAG | by Dr Julija | MediumOpen in appSign upSign inWriteSign upSign inLLM RAG Paradigms: Naive RAG, Advanced RAG & Modular RAGDr Julija¬∑Follow6 min read¬∑Mar 10, 2024--2ListenShareThree RAG Paradigms | üìî DrJulija‚Äôs Notebooküìù OverviewHere I describe my key learnings on how RAG systems evolved over the last few years. I share the differences between Naive RAG, Advanced RAG and Modular RAG frameworks. I summarize key insights from a great RAG technology survey paper Gao et al. 2024.üõ† What is a RAG Framework?Large Language Models (LLMs) such as the GPT series from OpenAI, LLama series by Meta, and Gemini by Google have achieved significant achievements in the generative AI field.But these models are non deterministic. Often, LLMs may produce content that is either inaccurate or irrelevant (known as hallucinations), rely on outdated information, and their decision-making processes are not transparent, leading to black-box\n",
      "\n",
      "Source: https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\n",
      "\n",
      "Document 3:\n",
      "LLM RAG Paradigms: Naive RAG, Advanced RAG & Modular RAG | by Dr Julija | MediumOpen in appSign upSign inWriteSign upSign inLLM RAG Paradigms: Naive RAG, Advanced RAG & Modular RAGDr Julija¬∑Follow6 min read¬∑Mar 10, 2024--2ListenShareThree RAG Paradigms | üìî DrJulija‚Äôs Notebooküìù OverviewHere I describe my key learnings on how RAG systems evolved over the last few years. I share the differences between Naive RAG, Advanced RAG and Modular RAG frameworks. I summarize key insights from a great RAG technology survey paper Gao et al. 2024.üõ† What is a RAG Framework?Large Language Models (LLMs) such as the GPT series from OpenAI, LLama series by Meta, and Gemini by Google have achieved significant achievements in the generative AI field.But these models are non deterministic. Often, LLMs may produce content that is either inaccurate or irrelevant (known as hallucinations), rely on outdated information, and their decision-making processes are not transparent, leading to black-box\n",
      "\n",
      "Source: https://medium.com/@drjulija/what-are-naive-rag-advanced-rag-modular-rag-paradigms-edff410c202e\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the relevant results with metadata\n",
    "print_relevante_docs(relevant_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3_rag_conversational"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "model = OllamaLLM(base_url=\"http://localhost:11434\", model=\"llama3.2\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "You are an assistant for question-answering tasks. \n",
    "Your name is {name}. \n",
    "Use the following pieces of retrieved context to answer the question.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "Context: {context}\n",
    "\"\"\"),\n",
    "    (\"placeholder\", \"{context}\"),\n",
    "    (\"human\", \"Question: {question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_with_partial = prompt.partial(name=\"R2D2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_with_partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "      prompt_with_partial\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = chain.invoke({\n",
    "    \"question\":\"Tell me about Naive RAG, Advanced RAG & Modular RAG in LLM RAG Paradigms\",\n",
    "    \"context\": []\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beep boop, I don't have specific information on Naive RAG, Advanced RAG, and Modular RAG within the provided context. However, I can tell you that these are related to Large Language Model (LLM) paradigms, specifically in the context of RAG (Recurrent Autoencoder for Graphs). Beep boop, more information would be needed to provide a detailed answer.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(documents):\n",
    "    return [AIMessage(doc.page_content) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='LLM RAG Paradigms: Naive RAG, Advanced RAG & Modular RAG | by Dr Julija | MediumOpen in appSign upSign inWriteSign upSign inLLM RAG Paradigms: Naive RAG, Advanced RAG & Modular RAGDr Julija¬∑Follow6 min read¬∑Mar 10, 2024--2ListenShareThree RAG Paradigms | üìî DrJulija‚Äôs Notebooküìù OverviewHere I describe my key learnings on how RAG systems evolved over the last few years. I share the differences between Naive RAG, Advanced RAG and Modular RAG frameworks. I summarize key insights from a great RAG technology survey paper Gao et al. 2024.üõ† What is a RAG Framework?Large Language Models (LLMs) such as the GPT series from OpenAI, LLama series by Meta, and Gemini by Google have achieved significant achievements in the generative AI field.But these models are non deterministic. Often, LLMs may produce content that is either inaccurate or irrelevant (known as hallucinations), rely on outdated information, and their decision-making processes are not transparent, leading to black-box', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='LLM RAG Paradigms: Naive RAG, Advanced RAG & Modular RAG | by Dr Julija | MediumOpen in appSign upSign inWriteSign upSign inLLM RAG Paradigms: Naive RAG, Advanced RAG & Modular RAGDr Julija¬∑Follow6 min read¬∑Mar 10, 2024--2ListenShareThree RAG Paradigms | üìî DrJulija‚Äôs Notebooküìù OverviewHere I describe my key learnings on how RAG systems evolved over the last few years. I share the differences between Naive RAG, Advanced RAG and Modular RAG frameworks. I summarize key insights from a great RAG technology survey paper Gao et al. 2024.üõ† What is a RAG Framework?Large Language Models (LLMs) such as the GPT series from OpenAI, LLama series by Meta, and Gemini by Google have achieved significant achievements in the generative AI field.But these models are non deterministic. Often, LLMs may produce content that is either inaccurate or irrelevant (known as hallucinations), rely on outdated information, and their decision-making processes are not transparent, leading to black-box', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='LLM RAG Paradigms: Naive RAG, Advanced RAG & Modular RAG | by Dr Julija | MediumOpen in appSign upSign inWriteSign upSign inLLM RAG Paradigms: Naive RAG, Advanced RAG & Modular RAGDr Julija¬∑Follow6 min read¬∑Mar 10, 2024--2ListenShareThree RAG Paradigms | üìî DrJulija‚Äôs Notebooküìù OverviewHere I describe my key learnings on how RAG systems evolved over the last few years. I share the differences between Naive RAG, Advanced RAG and Modular RAG frameworks. I summarize key insights from a great RAG technology survey paper Gao et al. 2024.üõ† What is a RAG Framework?Large Language Models (LLMs) such as the GPT series from OpenAI, LLama series by Meta, and Gemini by Google have achieved significant achievements in the generative AI field.But these models are non deterministic. Often, LLMs may produce content that is either inaccurate or irrelevant (known as hallucinations), rely on outdated information, and their decision-making processes are not transparent, leading to black-box', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='LLM RAG Paradigms: Naive RAG, Advanced RAG & Modular RAG | by Dr Julija | MediumOpen in appSign upSign inWriteSign upSign inLLM RAG Paradigms: Naive RAG, Advanced RAG & Modular RAGDr Julija¬∑Follow6 min read¬∑Mar 10, 2024--2ListenShareThree RAG Paradigms | üìî DrJulija‚Äôs Notebooküìù OverviewHere I describe my key learnings on how RAG systems evolved over the last few years. I share the differences between Naive RAG, Advanced RAG and Modular RAG frameworks. I summarize key insights from a great RAG technology survey paper Gao et al. 2024.üõ† What is a RAG Framework?Large Language Models (LLMs) such as the GPT series from OpenAI, LLama series by Meta, and Gemini by Google have achieved significant achievements in the generative AI field.But these models are non deterministic. Often, LLMs may produce content that is either inaccurate or irrelevant (known as hallucinations), rely on outdated information, and their decision-making processes are not transparent, leading to black-box', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='in the RAG space. RAG systems can be split into 3 categories:Naive RAGAdvanced RAGModular RAGSee the comparison between all three paradigms of RAG ‚Äî Naive RAG, Advanced RAG and Modular RAG below.Comparison between the three paradigms of RAG (Gao et al. 2024)1Ô∏è‚É£ Naive RAGThe Naive RAG pipeline consists of the below key phases:Data IndexingData Loading: This involves importing all the documents or information to be utilized.Data Splitting: Large documents are divided into smaller pieces, for instance, sections of no more than 500 characters each.Data Embedding: The data is converted into vector form using an embedding model, making it understandable for computers.Data Storing: These vector embeddings are saved in a vector database, allowing them to be easily searched.RetrievalWhen a user asks a question:The user‚Äôs input is first transformed into a vector (query vector) using the same embedding model from the Data Indexing phase.This query vector is then matched against all vectors in the', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_docs(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt_with_partial\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = rag_chain.invoke(\"Summarize anything you know about about Naive RAG, Advanced RAG & Modular RAG in LLM RAG Paradigms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a summary of the three paradigms mentioned:\n",
      "\n",
      "**Naive RAG**: The Naive RAG paradigm is one of the three categories of RAG (Retrieval-Augmented Generation) systems. It involves importing all relevant documents or information, splitting them into smaller pieces, converting the data into vector form using an embedding model, and storing these vector embeddings in a database for easy retrieval.\n",
      "\n",
      "**Advanced RAG**: Unfortunately, there is no detailed explanation provided about Advanced RAG in the text snippet. However, based on general knowledge, it's likely that Advanced RAG builds upon the Naive RAG paradigm by incorporating additional features or techniques to improve its performance and efficiency.\n",
      "\n",
      "**Modular RAG**: Similarly, Modular RAG is not explained in detail in the text snippet. However, based on general knowledge, it's possible that Modular RAG involves a more modular approach to building RAG systems, where different components are designed to work together seamlessly to achieve better results.\n",
      "\n",
      "In general, RAG systems aim to improve the performance of large language models (LLMs) by incorporating retrieval mechanisms that allow them to access relevant information from a database or external sources. The three paradigms mentioned above represent different approaches to building these retrieval-augmented generation systems.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
